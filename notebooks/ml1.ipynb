{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course is being taught by Jeremy Howard, and was developed by Jeremy along with Rachel Thomas. You can find the original notebook and video lectures at [Introduction to Machine Learning for Coders!](http://course18.fast.ai/ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People wanting to learn Python 3 as a first programming language, should have to take a look at \n",
    "[Learn Python 3 the Hard Way](https://www.amazon.com/gp/product/0134692888). If you're already a general purpose developer, but with no previous Python background, I recommend this book [Python Tricks: A Buffet of Awesome Python Features](https://www.amazon.es/Python-Tricks-Buffet-Awesome-Features/dp/1775093301). The more familiarity you have with numeric programming in Python, the better. If you're looking to improve in this area, we strongly suggest Wes McKinney's [Python for Data Analysis, 2nd ed](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/ref=asap_bc?ie=UTF8).\n",
    "\n",
    "For machine learning with Python, we also recommend these books:\n",
    "- [Introduction to Machine Learning with Python](https://www.amazon.com/Introduction-Machine-Learning-Andreas-Mueller/dp/1449369413): From one of the scikit-learn authors, which is the main library we'll be using\n",
    "- [Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow, 2nd Edition](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939/ref=dp_ob_title_bk): New version of a very successful book. A lot of the new material however covers deep learning in Tensorflow, which isn't relevant to this course\n",
    "- [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=MBV2QMFH3EZ6B3YBY40K)\n",
    "\n",
    "and some videos:\n",
    "\n",
    "- [Practical Deep Learning for Coders, v3](https://course.fast.ai/videos/?lesson=1) Hands on deep learning course based on pytorch and fast.ai\n",
    "- [Full Stack Deep Learning](https://fullstackdeeplearning.com/march2019#) Guide from start to end on how to develop a machine learning project\n",
    "\n",
    "Finally, I recommend [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf) (free ebook) for a math & statistics approach to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, datetime\n",
    "sys.path.insert(0,'../src')\n",
    "from utils import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from pathlib import Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_data = Path('..') / 'data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PATH_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir {PATH_data / 'raw'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to *Blue Book for Bulldozers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the [Blue Book for Bulldozers](https://www.kaggle.com/c/bluebook-for-bulldozers) Kaggle Competition: \"The goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\"\n",
    "\n",
    "This is a very common type of dataset and prediciton problem, and similar to what you may see in your project or workplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ...Kaggle Competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Kaggle](https://www.kaggle.com/) is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills.  There is nothing like being able to get hands-on practice and receiving real-time feedback to help you improve your skills.\n",
    "\n",
    "Kaggle provides:\n",
    "\n",
    "1. Interesting data sets\n",
    "2. Feedback on how you're doing\n",
    "3. A leader board to see what's good, what's possible, and what's state-of-art.\n",
    "4. Blog posts by winning contestants share useful tips and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle provides info about some of the fields of our dataset; on the [Kaggle Data info](https://www.kaggle.com/c/bluebook-for-bulldozers/data) page they say the following:\n",
    "\n",
    "For this competition, you are predicting the sale price of bulldozers sold at auctions. The data for this competition is split into three parts:\n",
    "\n",
    "- **Train.csv** is the training set, which contains data through the end of 2011.\n",
    "- **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012. You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
    "- **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n",
    "\n",
    "The key fields are in train.csv are:\n",
    "\n",
    "- SalesID: the unique identifier of the sale\n",
    "- MachineID: the unique identifier of a machine.  A machine can be sold multiple times\n",
    "- saleprice: what the machine sold for at auction (only provided in train.csv)\n",
    "- saledate: the date of the sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_raw = pd.read_csv(PATH_data / 'raw' / 'Train.csv', low_memory=False, parse_dates=[\"saledate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any sort of data science work, it's **important to look at your data**, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_all(df_raw.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe(percentiles=[.5]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes is very useful to slide the data, or apply some transformation to the data. Most of this kind of operations can be done using the `pandas` library. Take a look to the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# return an slice of columns\n",
    "df_raw[['SalesID','SalePrice']].head(10)\n",
    "# it's equivalent to\n",
    "#df_raw.loc[:,['SalesID','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return an slice of rows and columns\n",
    "df_raw.loc[10:20, ['SalesID','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where filtering\n",
    "df_raw[(df_raw.fiModelDesc=='35NX2') & (df_raw.datasource==132)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by column and aggregation\n",
    "df_raw[['YearMade','SalesID']].groupby('YearMade').count().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[['YearMade','UsageBand','SalesID']].groupby('YearMade').agg({'SalesID': 'count','UsageBand': ['count','size','nunique']}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[['YearMade','UsageBand']].groupby('YearMade').agg({'UsageBand': set}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Practical Exercise</span>: Calculate: \n",
    "* (min,mean,max) Sale Price by YearMade\n",
    "* age of each vehicle (datetime.date.today().year - YearMade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note what metric is being used for a project. Generally, selecting the metric(s) is an important part of the project setup. However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) between the actual and predicted auction prices. Therefore we take the log of the prices, so that RMSE will give us what we need (in the following section we will explain RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.distplot(df_raw.SalePrice, ax=axes[0], kde=False, label = 'SalePrice')\n",
    "sns.distplot(np.log(df_raw.SalePrice),ax=axes[1], kde=False, label = 'log SalePrice');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.SalePrice = np.log(df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "# The following code is supposed to fail due to string values in the input data\n",
    "m.fit(df_raw.drop('SalePrice', axis=1), df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a mix of **continuous** and **categorical** variables.\n",
    "\n",
    "The method `add_datepart` extracts particular date fields from a complete datetime for the purpose of constructing categoricals.  You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? add_datepart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_datepart(df_raw, 'saledate')\n",
    "df_raw.filter(regex=(\"sale.*\")).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will inspect the dataframe column types. Most machine learning models require that non-numerical columns should be transformed to numerical. This is another easy transformation we can get using the Categorical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_columns_by_type = df_raw.columns.to_series().groupby(df_raw.dtypes).groups\n",
    "df_raw_columns_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always inspect your data for missing values (NA's). In most common machine learning algorithms, the model doesn't accepts this kind of data, so you need to either remove it or transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df_raw.isnull().sum().sort_index()/len(df_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot (df_raw.UsageBand);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot (df_raw.UsageBand.fillna('Empty'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.UsageBand.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order convert a column to Categorical, we can use the `astype('category')` pandas method. The resulting column will contain an index that references the original value of the data. This transformation also takes care of all missing data, which is assigned the category code -1 (we can specify the order to use for categorical variables if we wish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_band_categories = df_raw.UsageBand.astype('category')\n",
    "usage_band_categories.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot (usage_band_categories.cat.codes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's save this file for now, since it's already in format can we be stored and accessed efficiently (in this case, we choose parquet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_parquet(PATH_data / 'processed' / 'train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can simply read it from this fast format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_raw = pd.read_parquet(PATH_data / 'processed' / 'train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_cat = df_raw.select_dtypes('object').head().columns.values.tolist()\n",
    "columns_cont = df_raw.columns.symmetric_difference(columns_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns_cat:\n",
    "    df_raw[col] = df_raw[col].astype('category').cat.codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_raw.dtypes.to_frame().applymap(str).reset_index().groupby(0).agg({'index':list}).to_dict()['index']\n",
    "df_raw.columns.to_series().groupby(df_raw.dtypes).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still not quite done - for instance we have some missing values, which we can't pass directly to a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_all(df_raw.isnull().sum()/len(df_raw))\n",
    "df_raw.columns[df_raw.isnull().sum() > 0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we replace NA's by the median of the column, but there exists more strategies to deal with NA's ( _imputation methods_ ):\n",
    "- Remove row / column\n",
    "- **Replace by** Zero / Mean / **Median** / Mode\n",
    "- Linear regression\n",
    "- Soft imputation\n",
    "- Add boolean column isNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.fillna(df_raw.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df_raw.isnull().sum() == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_parquet(PATH_data / 'interim' / 'train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have something we can pass to a random forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(df_raw, df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `score()` of the model returns the R^2 metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.score(df_raw, df_raw.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Practical Exercise</span>: The score is too good to be true. Search on the internet about the term [**data leakage**](https://www.google.com/search?q=data+leakage) (or if you're too lazy, click on [link for the lazies](https://insidebigdata.com/2014/11/26/ask-data-scientist-data-leakage/)). What is it? Now, review the training code and fix it to avoid the data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate machine learning models using **scoring metrics**. As mentioned  before, the original Kaggle competition uses RMSLE, but for our purposes we will use another metric called R^2 (aka as R-squared or [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)). This metric measures the percentage of variance explained by the model, so the value of this metric ranges between 0 and 1 (the higher the better). The definition is as follows:\n",
    "\n",
    "$$r^2=1-\\frac{RSS}{TSS}=1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Practical Exercise</span>: RMSLE comes from [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation), which is a very common metric, defined below:\n",
    "> $$RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{n}}$$\n",
    "> Develop a function that accepts a random forest model (m), a dataframe containing the features (X_train) and another dataframe containing the labels (y_train), and returns the RMSE value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you fix the data leakage, you will get an R^2 score around 0.98 - that's great, right? Well, perhaps not...\n",
    "\n",
    "Possibly **the most important idea** (and one of the differences respect Statistical methods) in Machine Learning  is that of having separate training & validation data sets. As motivation, suppose you don't divide up your data, but instead use all of it.  And suppose you have lots of parameters:\n",
    "\n",
    "<img src=\"../references/overfitting.png\" alt=\"\" style=\"width: 70%\"/>\n",
    "\n",
    "The error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice.  Why is that?  If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n",
    "\n",
    "This illustrates how using all our data can lead to **overfitting**. A validation set helps diagnose this problem. [Underfitting and Overfitting](https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.drop(columns=['SalePrice'])\n",
    "y = df_raw.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? split_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = 12_000  # same as Kaggle's test set size\n",
    "n_trn = len(df)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(df, n_trn)\n",
    "y_train, y_valid = split_vals(y, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our model again, this time with separate training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def print_score(m):\n",
    "    output = {\n",
    "        'RMSE train' : rmse(m.predict(X_train), y_train),\n",
    "        'RMSE val'   : rmse(m.predict(X_valid), y_valid),\n",
    "        'R^2 train'  : m.score(X_train, y_train),\n",
    "        'R^2 val'    : m.score(X_valid, y_valid)\n",
    "    }\n",
    "    if hasattr(m, 'oob_score_'): \n",
    "        output['R^2 oob'] = m.oob_score_\n",
    "    \n",
    "    pprint.pprint (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a helping method named `print_score()` which display main metrics of the model. Besides the R^2 mentioned before, now we display also the RMSE.\n",
    "\n",
    "An R^2 in the high-80's isn't bad at all, but we can see from the validation set score that we're over-fitting badly. To understand this issue, let's simplify things down to a single small tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subsample = df_raw.sample(n=20_000)\n",
    "X_train = df_subsample.drop(columns=[\"SalePrice\"])\n",
    "y_train = df_subsample.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's define an **Stochastic model**, which represents a model with some degree of uncertainty, where each event involved in the model has an associated degree of probability. The opposity is a **Deterministic model**, where the output (prediction) is certain. In simple words, this means real life models are stochastic, and by this, some level of randomness is added in the implementation of a machine learning model. As result, executing two times the same training, returns different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve some degree of reproducibility by setting up a random seed in the frameworks / libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10, random_state=42)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10, random_state=42)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we disable bootstrap in random forest, we take out the \"randomness\" in selecting the rows, so the resulting tree takes all available samples (max_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the following tree, you need to install [GraphViz - Windows Packages](https://graphviz.gitlab.io/_pages/Download/Download_windows.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(m.estimators_[0], X_train, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an understanding of tree and its parameters.\n",
    "\n",
    "1. Feature name — Feature at every node is decided after selecting a feature from a subset of all features.\n",
    "Feature selection based on Mean Squared Error (${\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{n}}$) or gini (or information gain) (gini is commonly used because it is less computational complex).\n",
    "\n",
    "2. Split value — split value is decided after selecting a threshold value which gives highest MSE (regression) or gini (classification) for that split\n",
    "\n",
    "3. MSE (or gini) — It is basically deciding factor i.e. to select feature at next node , to pick best split value etc.\n",
    "\n",
    "4. Samples — Number of samples remaining at that particular node.\n",
    "\n",
    "5. Value — Mean value of the samples at this node. In case of regression, is the average of the labels under this tree / leaf. In case of classication, number of samples of each class remaining at that particular node (theoretically sum of these values equals Samples value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve is another tool commonly used to represent the fitness of the model. Usually plots the improvement of the model (accuracy / loss function) respect training time / number of samples / epochs or any other kind of measure that changes by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(m, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we create a bigger tree. With enogh depth in the tree, and with no bootstrapping at all, the random forest is able to memorize the whole dataset (overfitting the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth:None = nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "m = RandomForestRegressor(n_estimators=1, max_depth=None, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set result looks great! But the validation set is worse than our original model. This is why we need to use *bagging* of multiple trees to get more generalizable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(m, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest = Bag of Little Bootstraps\n",
    "\n",
    "To learn about bagging in random forests, let's start with our basic model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab the predictions for each individual tree (`estimators_`), and look at one example, taking the average of all trees and compare with the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preds.shape[0] == len(m.estimators_) == m.n_estimators\n",
    "assert preds.shape[1] == X_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,0], np.mean(preds[:,0]), m.predict([X_valid.iloc[0]])[0], y_valid.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we keep adding trees, the rmse score improves (one tree is not worth it, but all combined trees are worth it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve = pd.DataFrame({'Score (r2)': [metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(m.n_estimators)],\n",
    "                               'n_estimators': range(m.n_estimators)})\n",
    "sns.lineplot(y='Score (r2)', x='n_estimators', data=learning_curve);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve shows that the model improves with the size of the estimators, but this growing is faster at the beginning, and stalls towards the end. So the question is ... until when is it worth to keep adding estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this curve suggests that adding more trees isn't going to help us much. Let's check. (Compare this to our original model on a sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=80, n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(m.n_estimators)]);\n",
    "learning_curve = pd.DataFrame({'Score (r2)': [metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(m.n_estimators)],\n",
    "                               'n_estimators': range(m.n_estimators)})\n",
    "sns.lineplot(y='Score (r2)', x='n_estimators', data=learning_curve, label=\"learning curve\", legend=False)\n",
    "ax = plt.twinx()\n",
    "sns.lineplot(x=[10,20,40,80], y=[1,1.5,3,5], color=\"r\", ax=ax, label=\"training time\", legend=False)\n",
    "ax.figure.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag (OOB) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we've shown, we can't tell. However, random forests have a very clever trick called *out-of-bag (OOB) error* which can handle this (and more!)\n",
    "\n",
    "The idea is to calculate error on the training set, but only include the trees in the calculation of a row's error where that row was *not* included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set.\n",
    "\n",
    "This also has the benefit of allowing us to see whether our model generalizes, even if we only have a small amount of data so want to avoid separating some out to create a validation set.\n",
    "\n",
    "This is as simple as adding one more parameter to our model constructor. We print the OOB error last in our `print_score` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=15, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Subsampling  [`set_rf_samples`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It turns out that one of the easiest ways to avoid over-fitting is also one of the best ways to speed up analysis: *subsampling*. Let's return to using our full dataset, so that we can demonstrate the impact of this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid = split_vals(df_raw.drop(columns=['SalePrice']), n_trn)\n",
    "y_train, y_valid = split_vals(df_raw.SalePrice, n_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The basic idea is this: rather than limit the total amount of data that our model can access, let's instead limit it to a *different* random subset per tree. That way, given enough trees, the model can still see *all* the data, but for each individual tree it'll be just as fast as if we had cut down our dataset as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_rf_samples(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=10, n_jobs=-1, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since each additional tree allows the model to see more data, this approach can make additional trees more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#plot_learning_curve(m, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We revert to using a full bootstrap sample in order to show the impact of other over-fitting avoidance methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reset_rf_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree building parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a baseline for this full set to compare to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One factor of complexity is the estimator (tree) depth, so if we reduce it, the model will overfit less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=m.estimators_[0].tree_\n",
    "dectree_max_depth(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([dectree_max_depth(est.tree_) for est in m.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to reduce over-fitting is to grow our trees less deeply. We do this by specifying (with **`min_samples_leaf`**) that we require some minimum number of rows in every leaf node. This has two benefits:\n",
    "\n",
    "- There are less decision rules for each leaf node; simpler models should generalize better\n",
    "- The predictions are made by averaging more rows in the leaf node, resulting in less volatility\n",
    "- recommended values: `[1, 3, 5, 10, 25, 100]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([dectree_max_depth(est.tree_) for est in m.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([dectree_max_depth(est.tree_) for est in m.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also increase the amount of variation amongst the trees by not only use a sample of rows for each tree, but to also using a sample of *columns* for each *split*. We do this by specifying **`max_features`**, which is the proportion of features to randomly select from at each split.\n",
    "- recommended values: `[None, 0.5, sqrt]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([dectree_max_depth(est.tree_) for est in m.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Practical Exercise</span>: Check if you can get a better model, by changing the values of the hyperparameters `max_features`, `min_samples_leaf` and `n_estimators`. These parameters are well documented (run `? RandomForestRegressor` command), but you can also test using other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn docs [show an example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html) of different `max_features` methods with increasing numbers of trees - as you see, using a subset of features on each split requires using more trees, but results in better models. The green line represents trees that take all the features, but this approach get worse results: not always pays better to take all the data, but column sampling works much better.\n",
    "![sklearn max_features chart](http://scikit-learn.org/stable/_images/sphx_glr_plot_ensemble_oob_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best hyperparameters using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search is a term related to finding the best hyperparameters. We can preset some values to use, and then grid search tries every combination and calculates best improvements using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_features': [None, 0.5, 'sqrt'], \n",
    "              'min_samples_leaf': [1, 5, 10, 25, 100], \n",
    "              'n_estimators': [10, 25, 50],\n",
    "              'oob_score': [False] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(m, parameters, cv=3)\n",
    "%time gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=gs.best_estimator_.estimators_[0].tree_\n",
    "dectree_max_depth(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([dectree_max_depth(est.tree_) for est in gs.best_estimator_.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([dectree_max_depth(est.tree_) for est in gs.best_estimator_.estimators_]).describe(percentiles=[.5])[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "PATH_models = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(gs.best_estimator_, PATH_models / 'bulldozers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_loaded = load(PATH_models / 'bulldozers.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new notebook and train a model based on the file `TrainAndValid.csv`, which is zipped into the file `data/raw/bluebook-for-bulldozers.zip`. You don't need to repeat all the EDA steps, but you need to extract all feature engineering realised in this notebook and apply it to the new dataset in order to build a random forest model.\n",
    "1. Load TrainAndValid.csv into a dataframe (saving the file as parquet file is recommended)\n",
    "2. Apply feature engineering steps\n",
    "3. Split dataset in train / validation splits (validation size = 12000)\n",
    "4. Train a new model using best hyperparameters (you don't need to repeat hyperparameter search, just copy best hyperparameters)\n",
    "5. Score the dataset using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlcourse)",
   "language": "python",
   "name": "mlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
